# -*- coding: utf-8 -*-
"""Nlp_Project_Part1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lhZVS2gd6_QSMYXjdEAV6GhP_PTHwdZ7
"""

!pip install Wikipedia-API

import wikipediaapi

wiki = wikipediaapi.Wikipedia(
    user_agent='MyProjectName (merlin@example.com)',
        language='en',
        extract_format=wikipediaapi.ExtractFormat.WIKI
)

topics = ["code smell",
"legacy system",
"database engineering",
"software development lifecycle",
"rewriting",
"software modernization",
"Automated Refactoring Tools",
"API",
"Agile usability engineering",
"Agile Management",
"documentation",
"model driven architecture",
"quality engineering",
"business process reengineering",
"duplicate code",
"test suite",
"regression testing",
"software reliability testing",
"interoperability",
"security engineering",
"user interface",
"UX Refactoring",
"agile software development",
"Risk management",
"CI/CD",
"regulatory compliance",
"cognitive complexity",
"Reengineering",
"Reverse engineering",
"Scrum",
"Scrumban",
"DMS Software Reengineering Toolkit",
"History of software engineering",
"Reengineering",
"Requirements analysis",
"Data modeling",
"Software development process",
"Incremental development",
"Waterfall Model",
"Spiral model",
"Component Based Software Engineering",
"Dynamic Program-analysis",
"Software Metric",
"Dependence Analysis",
"Obfuscation",
"Software Prototyping",
"Rapid Prototyping",
"Maintainability",
"Software Archaeology",
"Digital Archaeology",
"Domain Driven Design",
"Code property graph",
"Data dependency",
"Static program Analysis",
"List of tools for static code analysis",
"Configuration Management",
"Program transformation",
"List of program transformation systems",
"Mining Software Repositories",
"Usability engineering",
"Code Review",
"Software Erosion",
"Semantic Analysis in Reengineering",
"Concurrency",
"Middleware",
"aspect Oriented Programming",
"metaobject",
"Cloud engineering",
"Specification Language",
"Data Warehousing",
"Knowledge extraction",
"Enterprise Integration Patterns",
"Data engineering",
"Engineering Ethics",
"Feature engineering",
"Software Fault Tolerance",
"Model Driven architecture",
"Continuous Deployment in Reengineering",
    "Cross-platform Compatibility in Software Reengineering",
    "Code Generation Tools and Techniques",
    "Parallelization Refactoring",
    "Internationalization and Localization in Reengineering",
    "DevOps Integration in Reengineering",
    "Fuzz Testing in Reengineering",
    "Blockchain Integration in Legacy Systems",
    "Quantitative Analysis of Code Quality",
    "Behavior-Driven Development (BDD) in Reengineering",
    "Containerization in Software Reengineering",
    "Immutable Infrastructure in Reengineering",
    "Gamification of Code Review in Reengineering",
    "Microservices Architecture in Reengineering",
    "Compliance as Code in Reengineering",
    "Machine Learning for Code Improvement",
    "Decomposition Techniques in Reengineering",
    "Dark Launching Strategies in Reengineering",
    "Distributed Version Control Systems in Reengineering",
    "Natural Language Interface for Code Navigation",
    "Reactive Programming in Reengineering",
    "Integration of Virtual Reality (VR) in Software Analysis",
    "Event-Driven Architecture in Reengineering",
    "Quantum Computing Implications in Reengineering",
    "Ethical Hacking for Security Assessment in Reengineering"
     "Object-Oriented Programming (OOP)",
    "Model-View-Controller (MVC) Architecture",
    "Version Control Systems (e.g., Git)",
    "Continuous Integration and Continuous Deployment (CI/CD)",
    "Test-Driven Development (TDD)",
    "Requirements Engineering",
    "Software Design Patterns",
    "Refactoring Techniques",
    "Code Review Best Practices",
    "Software Architecture",
    "Scrum Methodology",
    "DevOps Practices",
    "User Interface (UI) Design Principles",
    "Software Quality Assurance (SQA)",
    "Security in Software Engineering",
    "Big Data Engineering",
    "Microservices Architecture",
    "Artificial Intelligence (AI) in Software Engineering"
    "Software Project Management",
    "Database Design and Management",
    "Software Testing Strategies",
    "Configuration Management",
    "Human-Computer Interaction (HCI)",
    "Web Development Technologies (e.g., HTML, CSS, JavaScript)",
    "Mobile App Development",
    "Cloud Computing in Software Engineering",
    "Distributed Systems",
    "Software Maintenance and Evolution",
    "Empirical Software Engineering",
    "Artificial Intelligence (AI) Ethics in Software",
    "Software Metrics and Measurement",
    "Requirements Traceability",
    "Software Development Tools (e.g., IDEs, Debuggers)",
    "Software Licensing and Open Source",
    "Software Documentation Practices",
    "Cryptography in Software Engineering",
    "Model-Based Development",
    "Empathy-Driven Development",
    "Algorithm Design and Analysis",
    "Data Structures",
    "Object-Oriented Analysis and Design (OOAD)",
    "Software Requirements Specification (SRS)",
    "Software Development Methodologies",
    "Programming Paradigms",
    "Code Debugging Techniques",
    "Software Development Tools (e.g., IDEs, Version Control)",
    "Software Engineering Ethics",
    "Software Development Paradigms (e.g., Waterfall, Agile)",
    "Software Development Models (e.g., V-Model)",
    "Software Verification and Validation",
    "Software Configuration Management",
    "Software Maintenance",
    "Software Documentation Standards",
    "Software Development Life Cycle (SDLC) Models",
    "Unified Modeling Language (UML)",
    "Software Engineering Standards and Practices",
    "Code Versioning and Branching",
    "Software Development Best Practices",
     "Software Design Principles",
    "Software Testing Levels (e.g., Unit, Integration, System)",
    "Software Engineering Models (e.g., Spiral Model)",
    "Software Estimation Techniques",
    "Software Prototyping",
    "Requirements Elicitation Techniques",
    "Software Quality Metrics",
    "Software Deployment Strategies",
    "Software Performance Engineering",
    "User Stories in Agile Development",
     "Software requirements specification",
      "Requirements engineering",
    "Business requirements"


]

len(topics)

for key in wiki_data.keys():
    print(f"Topic: {key}")
    print("Content:")
    print(wiki_data[key])
    print("\n")

import numpy as np
wiki_data ={}

for topic in topics:
  page_py= wiki.page(topic)
  if page_py.exists():
    wiki_data[page_py.title]=page_py.text

len(wiki_data)

for key in wiki_data.keys():
  print(key)

import re
import nltk
from nltk.stem import WordNetLemmatizer
nltk.download("wordnet")
nltk.download("omw-1.4")

lemmatizer = WordNetLemmatizer()



def clean_wiki_text(text):



  text = re.sub(r"<[^>]*>", "", text)


  text = re.sub(r"\[[^\]]*\]", "", text)

  text = re.sub('https?://\S+|www\.\S+', '', text)


  text = re.sub(r"[^a-zA-Z0-9\s]", " ", text)


  text = text.lower()




  text = re.sub(r"\s+", " ", text)


  text = text.strip()

  text = " ".join([lemmatizer.lemmatize(word) for word in text.split()])

  return text

cleaned_wiki_data = {}
for topic, content in wiki_data.items():
    cleaned_content = clean_wiki_text(content)
    cleaned_wiki_data[topic] = cleaned_content

print(cleaned_wiki_data["Code smell"])
print(cleaned_wiki_data["Legacy system"])
print(cleaned_wiki_data["Software development process"])
print(cleaned_wiki_data["Rewriting"])
print(cleaned_wiki_data["Software modernization"])
print(cleaned_wiki_data["API"])
print(cleaned_wiki_data["Agile usability engineering"])
print(cleaned_wiki_data["Documentation"])
print(cleaned_wiki_data["Quality engineering"])
print(cleaned_wiki_data["Business process re-engineering"])
print(cleaned_wiki_data["Duplicate code"])
print(cleaned_wiki_data["Test suite"])
print(cleaned_wiki_data["Regression testing"])
print(cleaned_wiki_data["Software reliability testing"])
print(cleaned_wiki_data["Interoperability"])
print(cleaned_wiki_data["Security engineering"])
print(cleaned_wiki_data["User interface"])
print(cleaned_wiki_data["Agile software development"])
print(cleaned_wiki_data["Risk management"])
print(cleaned_wiki_data["CI/CD"])
print(cleaned_wiki_data["Regulatory compliance"])
print(cleaned_wiki_data["Cognitive complexity"])
print(cleaned_wiki_data["Reengineering"])
print(cleaned_wiki_data["Reverse engineering"])
print(cleaned_wiki_data["Scrum"])
print(cleaned_wiki_data["Scrumban"])
print(cleaned_wiki_data["DMS Software Reengineering Toolkit"])
print(cleaned_wiki_data["History of software engineering"])
print(cleaned_wiki_data["Requirements analysis"])
print(cleaned_wiki_data["Data modeling"])
print(cleaned_wiki_data["Iterative and incremental development"])
print(cleaned_wiki_data["Waterfall model"])
print(cleaned_wiki_data["Spiral model"])
print(cleaned_wiki_data["Obfuscation"])
print(cleaned_wiki_data["Software prototyping"])
print(cleaned_wiki_data["Rapid prototyping"])
print(cleaned_wiki_data["Maintainability"])
print(cleaned_wiki_data["Digital archaeology"])
print(cleaned_wiki_data["Domain-driven design"])
print(cleaned_wiki_data["Code property graph"])
print(cleaned_wiki_data["Data dependency"])
print(cleaned_wiki_data["List of tools for static code analysis"])
print(cleaned_wiki_data["Configuration management"])
print(cleaned_wiki_data["Program transformation"])
print(cleaned_wiki_data["List of program transformation systems"])
print(cleaned_wiki_data["Mining software repositories"])
print(cleaned_wiki_data["Usability engineering"])
print(cleaned_wiki_data["Concurrency"])
print(cleaned_wiki_data["Middleware"])
print(cleaned_wiki_data["Aspect-oriented programming"])
print(cleaned_wiki_data["Metaobject"])
print(cleaned_wiki_data["Cloud engineering"])
print(cleaned_wiki_data["Data warehouse"])
print(cleaned_wiki_data["Knowledge extraction"])
print(cleaned_wiki_data["Enterprise Integration Patterns"])
print(cleaned_wiki_data["Data engineering"])
print(cleaned_wiki_data["Engineering ethics"])
print(cleaned_wiki_data["Feature engineering"])
print(cleaned_wiki_data["Software fault tolerance"])
print(cleaned_wiki_data["Requirements engineering"])
print(cleaned_wiki_data["Software architecture"])
print(cleaned_wiki_data["Distributed computing"])
print(cleaned_wiki_data["Empirical Software Engineering"])
print(cleaned_wiki_data["Requirements traceability"])
print(cleaned_wiki_data["Data structure"])
print(cleaned_wiki_data["Software configuration management"])
print(cleaned_wiki_data["Software maintenance"])
print(cleaned_wiki_data["Performance engineering"])
print(cleaned_wiki_data["Software requirements specification"])
print(cleaned_wiki_data["Business requirements"])

import nltk

# Download the 'punkt' resource
nltk.download('punkt')
nltk.download('stopwords')

from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords

# Assuming cleaned_wiki_data is a dictionary with topics as keys and cleaned content as values

# Combine all cleaned texts into a single string
all_text = ' '.join(cleaned_wiki_data.values())

# Tokenize the combined text into words
words = word_tokenize(all_text)

# Remove stopwords
stop_words = set(stopwords.words('english'))
filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]

# Calculate word count
word_count = len(filtered_words)

# Calculate vocabulary size
vocabulary_size = len(set(filtered_words))

print(f"Word Count: {word_count}")
print(f"Vocabulary Size: {vocabulary_size}")

import nltk
from nltk.tokenize import word_tokenize
from nltk.probability import FreqDist
from nltk.corpus import stopwords

# Download required resources
nltk.download('punkt')
nltk.download('stopwords')

# Assuming cleaned_wiki_data is a dictionary with topics as keys and cleaned content as values
vocab_sizes = {}
word_counts = {}

for topic, content in cleaned_wiki_data.items():
    # Tokenize the content into words
    words = word_tokenize(content)

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]

    # Calculate word count
    word_count = len(filtered_words)
    word_counts[topic] = word_count

    # Calculate vocabulary size
    vocabulary_size = len(set(filtered_words))
    vocab_sizes[topic] = vocabulary_size

# Print results for each topic
for topic in cleaned_wiki_data:
    print(f"Topic: {topic}")
    print(f"Word Count: {word_counts[topic]}")
    print(f"Vocabulary Size: {vocab_sizes[topic]}")
    print("\n")

import csv
import os
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
import re

# Download required resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

# Assuming cleaned_wiki_data is a dictionary with topics as keys and cleaned content as values

# Create a directory to store the CSV file
output_directory = "csv_output"
os.makedirs(output_directory, exist_ok=True)

# Define the CSV file path
csv_file_path = os.path.join(output_directory, "topics_data.csv")

# Function to clean and preprocess text
def clean_and_preprocess(text):
    # Remove HTML tags
    text = re.sub(r"<[^>]*>", "", text)

    # Remove square brackets and content inside them
    text = re.sub(r"\[[^\]]*\]", "", text)

    # Remove URLs
    text = re.sub('https?://\S+|www\.\S+', '', text)

    # Remove non-alphanumeric characters
    text = re.sub(r"[^a-zA-Z0-9\s]", " ", text)

    # Convert to lowercase
    text = text.lower()

    # Remove extra whitespaces
    text = re.sub(r"\s+", " ", text)

    # Strip leading and trailing whitespaces
    text = text.strip()

    # Lemmatize the words
    text = " ".join([lemmatizer.lemmatize(word) for word in text.split()])

    return text

# Open the CSV file for writing
with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:
    csv_writer = csv.writer(csv_file)

    # Write the header row
    csv_writer.writerow(["Topic", "Text", "Cleaned Text", "Word Count", "Vocabulary Size"])

    # Iterate through each topic and write the data to the CSV file
    for topic, content in cleaned_wiki_data.items():
        # Clean and preprocess the text
        cleaned_text = clean_and_preprocess(content)

        # Tokenize the cleaned text for word count and vocabulary size
        words = word_tokenize(cleaned_text)
        word_count = len(words)
        vocabulary_size = len(set(words))

        # Write the row for each topic
        csv_writer.writerow([topic, content, cleaned_text, word_count, vocabulary_size])

print(f"CSV file '{csv_file_path}' created successfully in the 'csv_output' directory.")

import csv
import os
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
from nltk.corpus import stopwords

# Download required resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Assuming cleaned_wiki_data is a dictionary with topics as keys and cleaned content as values

# Create a directory to store the CSV file
output_directory = "csv_output"
os.makedirs(output_directory, exist_ok=True)

# Define the CSV file path
csv_file_path = os.path.join(output_directory, "tokens.csv")

# Open the CSV file for writing
with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:
    csv_writer = csv.writer(csv_file)

    # Write the header row
    csv_writer.writerow(["Topic", "Stems", "Lemmas"])

    # Create stemmer and lemmatizer
    porter_stemmer = PorterStemmer()
    wordnet_lemmatizer = WordNetLemmatizer()

    # Iterate through each topic and write the data to the CSV file
    for topic, content in cleaned_wiki_data.items():
        # Tokenize the content into words
        words = word_tokenize(content)

        # Remove stopwords
        stop_words = set(stopwords.words('english'))
        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]

        # Calculate stems and lemmas
        stems = [porter_stemmer.stem(word) for word in filtered_words]
        lemmas = [wordnet_lemmatizer.lemmatize(word) for word in filtered_words]

        # Write the row for each topic
        csv_writer.writerow([topic, ' '.join(stems), ' '.join(lemmas)])

print(f"CSV file '{csv_file_path}' created successfully in the 'csv_output' directory.")



import csv
import os
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

# Download required resources
nltk.download('punkt')
nltk.download('stopwords')

# Assuming cleaned_wiki_data is a dictionary with topics as keys and cleaned content as values

# Create a directory to store the CSV file
output_directory = "csv_output"
os.makedirs(output_directory, exist_ok=True)

# Define the CSV file path
csv_file_path = os.path.join(output_directory, "vocabulary.csv")

# Open the CSV file for writing
with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:
    csv_writer = csv.writer(csv_file)

    # Write the header row
    csv_writer.writerow(["Topic", "Unique Words"])

    # Iterate through each topic and write the data to the CSV file
    for topic, content in cleaned_wiki_data.items():
        # Tokenize the content into words
        words = word_tokenize(content)

        # Remove stopwords
        stop_words = set(stopwords.words('english'))
        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]

        # Calculate unique words
        unique_words = set(filtered_words)

        # Write the row for each topic
        csv_writer.writerow([topic, ' '.join(unique_words)])

print(f"CSV file '{csv_file_path}' created successfully in the 'csv_output' directory.")

pip install textstat

import csv
import os
import nltk
import textstat
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import string
import re

# Download required resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()

# Assuming cleaned_wiki_data is a dictionary with topics as keys and cleaned content as values

# Create a directory to store the CSV file
output_directory = "csv_output"
os.makedirs(output_directory, exist_ok=True)

# Define the CSV file path for readability
readability_csv_file_path = os.path.join(output_directory, "readability.csv")

# Open the CSV file for writing
with open(readability_csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:
    csv_writer = csv.writer(csv_file)

    # Write the header row for readability
    csv_writer.writerow(["Topic", "Cleaned Text", "Flesch-Kincaid Grade Level"])

    # Iterate through each topic and write the readability data to the CSV file
    for topic, content in cleaned_wiki_data.items():
        # Clean and preprocess the text
        cleaned_text = clean_and_preprocess(content)

        # Compute Flesch-Kincaid Grade Level for cleaned text
        fk_grade_level = textstat.flesch_kincaid_grade(cleaned_text)

        # Write the row for each topic
        csv_writer.writerow([topic, cleaned_text, fk_grade_level])

print(f"CSV file '{readability_csv_file_path}' created successfully in the 'csv_output' directory.")

import csv
import os
import nltk
from nltk import pos_tag
from nltk.tokenize import word_tokenize

# Download required resources
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

# Assuming cleaned_wiki_data is a dictionary with topics as keys and cleaned content as values

# Create a directory to store the CSV file
output_directory = "csv_output"
os.makedirs(output_directory, exist_ok=True)

# Define the CSV file path
csv_file_path = os.path.join(output_directory, "pos.csv")

# Open the CSV file for writing
with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:
    csv_writer = csv.writer(csv_file)

    # Write the header row
    csv_writer.writerow(["Topic", "POS Tags"])

    # Iterate through each topic and write the data to the CSV file
    for topic, content in cleaned_wiki_data.items():
        # Tokenize the content into words
        words = word_tokenize(content)

        # Perform part-of-speech tagging
        pos_tags = pos_tag(words)

        # Convert the list of tuples to a string for writing to CSV
        pos_tags_str = ' '.join([f"{word}/{tag}" for word, tag in pos_tags])

        # Write the row for each topic
        csv_writer.writerow([topic, pos_tags_str])

print(f"CSV file '{csv_file_path}' created successfully in the 'csv_output' directory.")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Step 1: Read the data
readability_data = pd.read_csv("csv_output/readability.csv")

# Step 2: Encode the readability labels
readability_data['Readability'] = readability_data['Readability'].apply(lambda x: 0 if x == 'Easy' else 1)

# Check the unique values in the target variable
print("Unique values in 'Readability':", readability_data['Readability'].unique())

# Step 3: Split the data into features (X) and target variable (y)
X = readability_data[['Flesch-Kincaid Grade Level']]
y = readability_data['Readability']

# Check the unique values in the target variable after encoding
print("Unique values in 'y':", y.unique())

# Check the class distribution
print("Class distribution in 'y':", y.value_counts())

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Build the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
classification_report_result = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print("Classification Report:")
print(classification_report_result)